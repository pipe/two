<!doctype html>
<html lang="en">
<head>
    <title>Unrupt calls</title>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <style>
        @media (min-height: 199.99px) and (min-width: 149.99px) {
            .video {
                width: 240px;
                height: 320px;
                background: greenyellow;
            }
        }


        @media (min-height: 499.99px) and (min-width: 299.99px) {
            .video {
                width: 240px;
                height: 320px;
                background: gold;
            }
        }

        @media (min-height: 799.99px) and (min-width: 599.99px) {
            .video {
                width: 480px;
                height: 640px;
                background: goldenrod;

            }
        }
        @media (min-height: 1023.99px) and (min-width: 799.99px) {
            .video {
                width: 720px;
                height: 960px;
                background: darkgoldenrod;
            }
        }
        @media (min-height: 1299.99px) and (min-width: 999.99px) {
            .video {
                width: 960px;
                height: 1280px;
                background: darkolivegreen;
            }
        }

        #out {
            transform: rotateY(180deg);
            width:100%;
            position:absolute;
            left:0;
            right:0;
            bottom:0;
            margin:auto;
            background-size:contain;
            background-position: center;
        }

        #in {
            position: absolute;
            left:0px;
            top:0px;
            width: 96px;
            height: 128px;
        }


        #audioInfo {
            position: relative;

        }

        #otherUser {
            position: relative;

        }


        .playpause {
            display: none;
            width:100%;
            position:absolute;
            left:0;
            right:0;
            bottom:0;
            margin:auto;
            background-size:contain;
            background-position: center;
            z-index: 99999;
        }
    </style>

</head>
<body>
<!-- Optional JavaScript -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" crossorigin="anonymous"></script>
<script src="https://pi.pe/iot/js/qrcode.js"></script>

<a href="ios.html"><h1 id="title" class="text-center">Verbol Speak</h1></a>
    <div class="video" id="otherUser" onclick="doPlay();">
        <div id="pauseOther" class="playpause">
            <object data="pause.svg"></object>
        </div>
    </div>
<div class="container">
        <div class="progress">
            <div id="unruptbuffer_spk" class="progress-bar progress-bar-striped bg-warning" role="progressbar"
                 aria-valuenow="100" aria-valuemin="0" aria-valuemax="100"></div>
            <div id="unruptbuffer_sil" class="progress-bar " role="progressbar" aria-valuenow="100"
                 aria-valuemin="0" aria-valuemax="100"></div>
        </div>
    <div>
        <span id="mode" class="badge badge-primary pull-left">Direct</span>
        <span id="unruptbuffer_len" class="badge pull-right"></span>
    </div>
        <div id="chosenAction" class="row">
            <button id="action" type="button" class="btn btn-primary btn-lg"><i class="fa fa-sign-out fa-2x" aria-hidden="true"></i></button>
            <button id="pause" type="button" class="btn btn-primary btn-lg"><i id="pauseIcon" class="fa fa-play-circle fa-2x" aria-hidden="true"></i></button>
            <button id="mute" type="button" class="btn btn-primary btn-lg"><i id="muteIcon" class="fa fa-microphone fa-2x" aria-hidden="true"></i></button>
            <button id="unruptToggle" type="button" class="btn btn-primary btn-lg"><i id="pwsIcon" class="fa fa-exchange fa-2x" aria-hidden="true"></i></button>
        </div>
</div>
<div id="share" class="modal" tabindex="-1" role="dialog">
    <div class="modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title">Share this url</h5>
            </div>
            <div class="modal-body" id="shareQR">
                <p>Share this URL to request a call</p>
            </div>
            <div class="modal-footer">
                <button id="shareDone" type="button" class="btn btn-secondary" data-dismiss="modal" onclick="shared();">Done</button>
            </div>
        </div>
    </div>
</div>
<div id="accept" class="modal" tabindex="-1" role="dialog">
    <div class="modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title">Accept a call</h5>
            </div>
            <div class="modal-body">
                <p>Click 'accept' to start the call.</p>
            </div>
            <div class="modal-footer">
                <button id="callAccept" type="button" class="btn btn-secondary" data-dismiss="modal" onclick="accepted();">Accept</button>
            </div>
        </div>
    </div>
</div>

<!--script src="properties.js"></script-->
<script>
var properties = {
versionname :"Default", // update this to indicate which version of the settings this is
procFramesize: 4096, // how many samples (at 44k1 Hz) in a frame. default 4096 ->  ~100ms
// this impacts the latency - but go too low and the audio will break up
scopeFftSize : 2048, // number of samples in the FFT for the oscilloscope. default 2048 seems ok
micSilenceThreshold : 0.0175, // minumum mean mic volume (in range of 0.0->1.0) of a frame that contains voice
// used to trigger pause/unpause
// default 0.0175 ok on imacs
farSilenceThreshold : 0.0175, // minimum mean remote volume (in range of 0.0->1.0) of a frame that contains voice
// used to trim silence from playout
// default 0.0175 ok on imacs

minFramesSilenceForPause: 7, // number of non-silent frames before we pause. default 3 (300ms)
minFramesSilenceForPlay: 3,  // number of silent frames before we start to clip silence from playback. default 3 (300ms)
maxStashFrames: 1000, // longest possible pause (in frames). default 1000 -> 100 sec
websocketURL: "wss://pi.pe/websocket/?finger=" // where to find the redezvous server.
};



    var webRTCconfiguration = {
        "iceServers": [
            {urls: "stun:146.148.121.175:3478"},
            {urls: "turn:146.148.121.175:3478?transport=udp", 'credential': 'nexus5x', 'username': 'smartphone'},
        ],
        "bundlePolicy":"max-bundle","iceCandidatePoolSize":1
    };



    var pc; // actual peer connection to our friend
    var socket; // used to set up connection with our peer.
    var mid;
    var fid;
    var cid;
    var myac;
    var yourac;
    var yourBuffer;
    var myBuffer;
    var initiator;
    var lcandyStash = [];
    var rcandyStash = [];
    var localStream;
    var remoteStream;
    var scopes= [];
    var procs = [];
    var backlog = 0;
    var backlog_sil = 0;
    var backlog_spk = 0;
    var tick;
    var iamspeaking = false;
    var mute = false;
    var paused = false;
    var videoEnabled = false;
    var peerConnectionOfferAnswerCriteria =  {offerToReceiveAudio: true, offerToReceiveVideo: false };
    var unruptEnabled = true;
    var toggleUnrupt;
    var AudioContext = window.AudioContext || window.webkitAudioContext;
    var framecount =0;
    var mode = "waiting";

    // message stuff - used to create direct connection to peer over WEBRTC

    function messageDeal(event){
        //console.log("message is ", event.data);
        var data = JSON.parse(event.data);
        console.log("message data is ", data);
        if (data.to != mid){
            alert("message mixup");
        }
        switch (data.type) {
            case "cheatUnruptToggle":
                toggleUnrupt();
                break;
            case "offer":
                if (fid == null) {
                    fid = data.from;
                }
                pc.setRemoteDescription(data).then( _ =>
                    pc.createAnswer(peerConnectionOfferAnswerCriteria).then(ans =>
                        pc.setLocalDescription(ans).then( _ =>
                            sendMessage(fid, mid, "answer", ans.sdp)
                        )
                    )
                )
                    .catch((e) => console.log("set Remote offer error", e));
                break;
            case "answer":
                pc.setRemoteDescription(data)
                    .then( _ => {
                    //$("#action").text("hangup");
                })
                    .catch(e => console.log("set Remote answer error", e) );
                break;
            case "candidate":
                var jc = {
                    sdpMLineIndex: 0,
                    candidate: data.sdp
                };
                console.log("adding candidate ", jc);
                var nc = new RTCIceCandidate(jc);
                pc.addIceCandidate(nc)
                    .then( _ => console.log("added remote candidate"))
                    .catch((e) => console.log("couldn't add candidate ", e));
                break;
        }
    }

    function sendMessage(to,from,type,data){
        var messageJ = {
            to:to,
            from:from,
            type:type,
            sdp:data
        };

        var message = JSON.stringify(messageJ);
        console.log("sending ", messageJ);
        socket.send(message);
    }

    // button actions

    function startCall(cid){
        lcandyStash = [];
        rcandyStash = [];
        fid = cid;
        pc.createOffer(peerConnectionOfferAnswerCriteria)
            .then(desc => {
                console.log("offer created",);
                pc.setLocalDescription(desc).then( d => sendMessage(fid, mid, desc.type, desc.sdp));
            })
            .catch(e => console.log("offer not created due to ", e) );
    }

    function stopCall(){
        window.location.href = window.location.href;
    }

function videoCapture() {
    var video = document.getElementById("out");
    var videoWidth = video.videoWidth, videoHeight = video.videoHeight;
    var canvas = document.getElementById('pauseOther');
    canvas.width = videoWidth;
    canvas.height = videoHeight;
    var context = canvas.getContext('2d');
    context.translate(canvas.width, 0);

    // flip context horizontally
    context.scale(-1, 1);

    context.drawImage(
        video, 0, 0, videoWidth, videoHeight
    );

    var img = new Image();
    img.onload = function() {
        context.drawImage(img, (videoWidth/2)-24, (videoHeight/2)-24);
    }
    img.src = "pause.svg";
}
    // webaudio processing

    // display waveform for diagnostics
    function doScopeNode(ac, node, scopename) {
        console.log("making scope node ", scopename);
        var analyser = ac.createAnalyser();
        analyser.fftSize = properties.scopeFftSize;
        node.connect(analyser);
        makeDraw(scopename, analyser);
        scopes.push(analyser);
        return analyser;
    }

    // processing incoming audio
    function yourProc(node){
        var buffer = yourBuffer;
        console.log("made unrupt buffer of size ", buffer.bufferSize, buffer);
        var silentcount = 0;
        var audiostash = [];

        var ub = $('#unruptToggle');

        var pb = $("#pause");

        console.log('PauseButton!!!', pb);

        var oldmute = false;

        toggleUnrupt = () => {
            var ubi = $('#pwsIcon');
            if (unruptEnabled) {
                unruptEnabled = false;
                console.log('disconnecting the buffer');
                node.disconnect(buffer);
                document.getElementById('out').muted = false;
                document.getElementById('out').play();
                $('#pauseOther').hide();
                ubi.removeClass("fa-exchange");
                ubi.addClass("fa-arrows-h");
            } else {
                unruptEnabled = true;
                console.log('connecting the buffer');
                node.connect(buffer);
                document.getElementById('out').muted = true;
                ubi.removeClass("fa-arrows-h");
                ubi.addClass("fa-exchange");
            }
        }


        ub.off('click').on('click', (e) => {
            e.stopImmediatePropagation();
            toggleUnrupt();
            sendMessage(fid, mid, "cheatUnruptToggle", true);
        });

        pb.off('click').on('click', (e) => {
            e.stopImmediatePropagation();
            var pbi = $("#pauseIcon");
            console.log('clicked on pause!');

            console.log('paused variable is', paused);

            if (!paused){
                paused = true;
                pbi.removeClass("fa-play-circle");
                pbi.addClass("fa-pause-circle");
                oldmute = mute;
                setMute(true);
            } else {
                paused = false;
                pbi.removeClass("fa-pause-circle");
                pbi.addClass("fa-play-circle");
                setMute(oldmute);
            }
        });

        buffer.onaudioprocess = (ape) => {
            framecount++;
            if (!unruptEnabled) {
                console.log('unrupt isnt enabled so clearing the backlog numbers');
                backlog_sil = backlog_spk = 0;
                return;
            }

            var inputBuffer = ape.inputBuffer;
            // The output buffer contains the samples that will be modified and played
            var outputBuffer = ape.outputBuffer;

            // Loop through the output channels (in this case there is only one)
            if (inputBuffer.numberOfChannels == 1){
                var inputData = inputBuffer.getChannelData(0);
                var outputData = outputBuffer.getChannelData(0);

                // we (almost) always stash the inbound data
                if (audiostash.length < properties.maxStashFrames) {
                    var buff = new Float32Array(inputBuffer.length);
                    var avg = 0.0;

                    for (var sample = 0; sample < inputBuffer.length; sample++) {
                        buff[sample] = inputData[sample]; // copy
                        avg += Math.abs(buff[sample]); // sample
                    }
                    avg = avg / inputBuffer.length;
                    var silent =   (avg  < properties.farSilenceThreshold);
                    if (silent){
                        silentcount++;
                        backlog_sil++;
                    } else {
                        silentcount =0;
                        backlog_spk++;
                    }
                    audiostash.unshift({silent: silent, buff:buff, silentcount: silentcount});
                    // store annotated version
                }
                backlog = audiostash.length /10;
                if (paused || iamspeaking) {
                    for (var sample = 0; sample < inputBuffer.length; sample++) {
                        outputData[sample] = 0.0; // silence
                    }
                } else {
                    var stash = audiostash.pop();
                    if (stash.silent){
                        backlog_sil--;
                    } else {
                        backlog_spk--;
                    }

                    var deleteme = (stash.silent);

                    while ((audiostash.length > 0) && (deleteme)){
                        console.log("have backlog", audiostash.length);
                        deleteme = false;
                        if (audiostash[0].silent){
                            console.log("next one is silent too");
                            if (stash.silentcount > properties.minFramesSilenceForPlay){
                                console.log("silentcount = ", stash.silentcount);
                                deleteme = true;
                                stash = audiostash.pop();
                                if (stash.silent){
                                    backlog_sil--;
                                } else {
                                    backlog_spk--;
                                }
                            }
                        }
                    }
                    var buff = stash.buff;
                    for (var sample = 0; sample < inputBuffer.length; sample++) {
                        outputData[sample] = buff[sample];
                    }
                }
            }
        };
        node.connect(buffer);
        procs.push(buffer)
        return buffer;
    }

    // mute management
    function setMute(m){
        var mi = $("#muteIcon");
        mute = m;
        if (m){
            mi.removeClass("fa-microphone");
            mi.addClass("fa-microphone-slash");
        } else {
            mi.removeClass("fa-microphone-slash");
            mi.addClass("fa-microphone");
        }
        var audioTracks = localStream.getAudioTracks();
        if (audioTracks[0]) {
            audioTracks[0].enabled = !m;
        }
    }

    // processing audio from the local microphone
    function myProc(node){
        var mb = $("#mute");
        mb.click( () => {
            setMute(!mute);
        });
        var buffer = myBuffer;
        console.log("made unrupt buffer of size ", buffer.bufferSize);
        var silentcount = 0;
        buffer.onaudioprocess = ape => {
            var inputBuffer = ape.inputBuffer;

            var outputBuffer = ape.outputBuffer;

            // Loop through the output channels (in this case there is only one)
            if (inputBuffer.numberOfChannels == 1){
                var inputData = inputBuffer.getChannelData(0);
                var outputData = outputBuffer.getChannelData(0);
                // Loop through the 4096 samples
                var avg = 0.0;

                for (var sample = 0; sample < inputBuffer.length; sample++) {
                    // make output equal to the same as the input
                    outputData[sample] = inputData[sample];
                    avg += Math.abs(outputData[sample]); // sample
                }
                avg = avg / inputBuffer.length;
                var silent =   (avg  < properties.micSilenceThreshold);
                if (iamspeaking){
                    if (silent){
                        silentcount++;
                    }
                    if (silentcount > properties.minFramesSilenceForPause){
                        iamspeaking = false;
                    }
                } else {
                    if (!silent) {
                        iamspeaking = true;
                        silentcount =0;
                    }
                }
            }
        };
        node.connect(buffer);
        procs.push(buffer)
        return buffer;
    }

    // called when webRTC presents us with a fresh remote audio stream
    function addStream(stream,kind) {
        if (!kind) {
            kind = "audio/video";
        }

        console.log("got new stream" + stream + " kind =" + kind);
        if (kind.indexOf("video") != -1) {
            remoteStream = stream;
            var mediaElement = document.getElementById('out');
            mediaElement.srcObject = stream;
            //mediaElement.muted = true;
            console.log('Video stream');

            mediaElement.onloadedmetadata = function (e) {
                //mediaElement.play();
                mediaElement.muted = true;
            };
        }
        if (kind.indexOf("audio")!= -1) {
            var peer = yourac.createMediaStreamSource(stream);

            console.log('Audio sample Rate is '+ yourac.sampleRate);

            var scope = doScopeNode(yourac, peer, "farscope");
            var buffproc = yourProc(scope);
            var scope2 = doScopeNode(yourac, buffproc, "earscope");
            scope2.connect(yourac.destination);
            $("#chosenAction").show();
        }
    }

    // configure local peerconnection and handlers
    function setupRTC(){
        pc = new RTCPeerConnection(webRTCconfiguration, null);
        console.log("created peer connection");

        pc.onicecandidate = (e) => {
            console.log("local ice candidate", e.candidate);
            if (e.candidate != null) {
                if (pc.signalingState == 'stable') {
                    sendMessage(fid, mid, "candidate", e.candidate.candidate);
                } else {
                    console.log("stashing ice candidate");
                    lcandyStash.push(e.candidate);
                }
            }
        };
        pc.oniceconnectionstatechange = (e) => {
            console.log("ice state is changed", pc.iceConnectionState);
            if (pc.iceConnectionState === "failed"){
                stopCall();
            }
        };
        // specification of WEBRTC is in flux - so we test to see if ontrack callback exists
        if ('ontrack' in pc) {
            // if so we use it
            pc.ontrack = (event) => {
                var stream = event.streams[0];
                console.log("got remote track ", event.track.kind);
                addStream(stream,event.track.kind);
            };
        } else {
            // if not we use add stream instead
            pc.onaddstream = (event) => {
                var stream = event.stream;
                console.log("got remote stream ", stream.kind);
                addStream(stream);
            }
        }

        // use this to determine the state of the 'hangup' button and send any candidates we found quickly
        pc.onsignalingstatechange = (evt) => {
            console.log("signalling state is ", pc.signalingState);
            if (pc.signalingState == 'stable') {
                var can;
                while (can = lcandyStash.pop()) {
                    console.log("popping candidate off stash")
                    sendMessage(fid, mid, "candidate", can.candidate);
                }
                var act = $("#action");
                //act.text("hangup");
                act.click( _ => stopCall() );
            }
        };
    }

    // plumb the local audio together.
function setupAudio() {

    myac = new AudioContext();
    yourac = new AudioContext();

    yourBuffer = yourac.createScriptProcessor(properties.procFramesize, 1, 1);
    myBuffer = myac.createScriptProcessor(properties.procFramesize, 1, 1);
    let supportedConstraints = navigator.mediaDevices.getSupportedConstraints();
    console.log("Supported constraints");
    for (let constraint in supportedConstraints) {
        if (supportedConstraints.hasOwnProperty(constraint)) {
            console.log("\t" + constraint + "=" + supportedConstraints[constraint]);
        }
    }
    //var gumConstraints = {audio: true, video: videoEnabled ? {width: 640, height: 480} : false};
    var gumConstraints = {audio: true, video: true};

    var promise = new Promise(function (resolve, reject) {

        navigator.mediaDevices.getUserMedia(gumConstraints)
            .then((stream) => {
                localStream = stream; // in case we need it
                var node = myac.createMediaStreamSource(stream);
                var detect = myProc(node);
                var manl = doScopeNode(myac, detect, "nearscope");
//                var dest = myac.createMediaStreamDestination();
//                manl.connect(dest);
//                var lstream = dest.stream;
                if (pc.addTrack) {
                    stream.getTracks().forEach(track => {
                        pc.addTrack(track, stream);
                        console.log("added local track ", track.id, track.kind);
                    });
                } else {
                    pc.addStream(stream);
                    console.log("added local stream");
                }
                if (videoEnabled) {
                    var ourMediaElement = document.getElementById('in');
                    ourMediaElement.srcObject = stream;

                    ourMediaElement.onloadedmetadata = function (e) {
                        //ourMediaElement.play();
                    };
                    ourMediaElement.onclick = function (e) {
                        console.log("onclick for in video");
                        ourMediaElement.play();
                        var theirMediaElement = document.getElementById('out');
                        theirMediaElement.play();
                    };
                }
                resolve(videoEnabled);
            })
            .catch((e) => {
                console.log('getUserMedia() error:' + e);
                reject(e);
            });
    });
    return promise;

}

function doPlay() {
    var ourMediaElement = document.getElementById('in');

    console.log("onclick for in video");
    ourMediaElement.play();
    var theirMediaElement = document.getElementById('out');
    theirMediaElement.play();
}


function shared() {
    setupRTC();
    setupAudio().then(_ => {
        console.log("ready for offer");
        doPlay();

    });
}
function accepted() {
    setupRTC();
    setupAudio().then(_ => {
        console.log("ready for offer");
        startCall(cid)
        doPlay();
    });
}
// decide who we are initiator or recpient.
// notice that the actual call goes in the reverse direction
// the recipient of the invite actually creates the audiobearing peerconnection
// the initiator then accepts this audio - This allows the initiator the chance to
// change their mind, if their circumstances have changed since the invite was sent.

    function setRole() {
        cid = $.getUrlVar("unruptId");
        mid = localStorage['unruptId'];
        //var act = $("#action");
        if (!mid) {
            var array = new Uint32Array(8);
            window.crypto.getRandomValues(array);
            var hexCodes = [];
            for (var i = 0; i < array.length; i ++ ){
                // Using getUint32 reduces the number of iterations needed (we process 4 bytes each time)
                var value = array[i];
                // toString(16) will give the hex representation of the number without padding
                var stringValue = value.toString(16)
                // We use concatenation and slice for padding
                var padding = '00000000'
                var paddedValue = (padding + stringValue).slice(-padding.length)
                hexCodes.push(paddedValue);
            }
            mid = hexCodes.join("").toLowerCase();
            console.log("mid =", mid);
            localStorage['unruptId'] = mid;
        }
        if (cid == null){
            document.location = location.pathname + "?" + "unruptId=" + mid + "&video=1";
            // this has the effect of getting our id into the browser bar -
            // making it easy to share etc.
        } else {
            var qrcode = new QRCode(document.getElementById("shareQR"), {
                width: 280,
                height: 280,
                correctLevel: QRCode.CorrectLevel.L
            });
            socket = new WebSocket( properties.websocketURL + mid);
            socket.onmessage = messageDeal;
            socket.onopen = (_) => {
                var url = document.location.href;
                console.log("href = "+url);
                if (url) {
                    qrcode.makeCode(url);
                }
                initiator = (mid === cid);
                var smodal = initiator ? "#share":"#accept";
                $(smodal).modal('show');
            };
            socket.onerror = (e) => {
                console.log("can't open websocket", e);
            };
            socket.onclose = (e) => {
                console.log(" websocket closed", e);
            };

        }
    }

    // thing that draws the scopes...
    function makeDraw(canvName, anode) {
        var analyser = anode;
        var speaking = false;
        var bufferLength = analyser.frequencyBinCount;
        var dataArray = new Uint8Array(bufferLength);
        var canvas = document.getElementById(canvName);
        var badge = document.getElementById(canvName + "-badge");
        if ((badge) && (canvas)) {
            var canvasCtx = canvas.getContext("2d");
// oscilloscope - for debug.
            var draw = _ => {
// Get a canvas defined with ID "oscilloscope"
                var drawVisual = requestAnimationFrame(draw);

                analyser.getByteTimeDomainData(dataArray);

                canvasCtx.fillStyle = 'rgb(200, 200, 200)';
                canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

                canvasCtx.lineWidth = 2;
                canvasCtx.strokeStyle = 'rgb(0, 0, 0)';

                canvasCtx.beginPath();

                var sliceWidth = canvas.width * 1.0 / bufferLength;
                var x = 0;
                var tot = 0.0;

                for (var i = 0; i < bufferLength; i++) {


                    var v = dataArray[i] / 128.0;
                    tot += Math.abs(dataArray[i] - 128);
                    var y = v * canvas.height / 2;

                    if (i === 0) {
                        canvasCtx.moveTo(x, y);
                    } else {
                        canvasCtx.lineTo(x, y);
                    }

                    x += sliceWidth;
                }
                var mean = tot / bufferLength;
                var newspeak = (mean > 2.0);
                if (newspeak != speaking) {
                    speaking = newspeak;
                    //console.log("newspeak "+newspeak+" mean "+mean);
                    badge.innerText = speaking ? "Speaking" : "Silent";
                }
                canvasCtx.lineTo(canvas.width, canvas.height / 2);
                canvasCtx.stroke();
            };
            draw();
        }
    }



    // some housekeeping

    $.extend({
        getUrlVars: function(){
            var vars = [], hash;
            var hashes = window.location.href.slice(window.location.href.indexOf('?') + 1).split('&');
            for(var i = 0; i < hashes.length; i++)
            {
                hash = hashes[i].split('=');
                vars.push(hash[0]);
                vars[hash[0]] = hash[1];
            }
            return vars;
        },
        getUrlVar: function(name){
            return $.getUrlVars()[name];
        }
    });

    $( document ).ready( _ => {
        videoEnabled = $.getUrlVar("video") == '1';

        if(videoEnabled) {
            peerConnectionOfferAnswerCriteria = {};
            var otherUserMediaElement = document.createElement('video');
            otherUserMediaElement.id = 'out';
            otherUserMediaElement.muted = true;
            otherUserMediaElement.className ="video";
            otherUserMediaElement.setAttribute("playsinline", "true");
            //otherUserMediaElement.setAttribute("autoplay", "true");


            document.getElementById('otherUser').appendChild(otherUserMediaElement);

            var thisUserMediaElement = document.createElement('video');
            thisUserMediaElement.id = 'in';
            thisUserMediaElement.muted = true;
            thisUserMediaElement.setAttribute("playsinline", "true");
            //thisUserMediaElement.setAttribute("autoplay", "true");


            document.getElementById('otherUser').appendChild(thisUserMediaElement);


        } else {
            var mediaElement = document.createElement('audio');
            mediaElement.id = 'out';
            mediaElement.muted = true;
            document.body.appendChild(mediaElement);
        }
        $("#chosenAction").hide();
        setRole();

        //$('#version').text(properties.versionname);
        tick = window.setInterval( t => {
            var scale = properties.maxStashFrames / 100.0;
            var spk = backlog_spk / scale;
            var sil = backlog_sil / scale;
            var timeline = Math.floor((backlog_spk + backlog_sil) * properties.procFramesize / 44100.0);
            $('#unruptbuffer_len').text(timeline + " seconds");
            $('#unruptbuffer_sil').css('width', sil + "%").attr('aria-valuenow', sil);
            $('#unruptbuffer_spk').css('width', spk + "%").attr('aria-valuenow', spk);
            var playout = backlog > 1 ? "playing" : "direct";
            var newmode = (iamspeaking || paused) ? "paused" : playout;
            if (newmode != mode) {
                if (remoteStream && videoEnabled && unruptEnabled) {
                    mode = newmode;
                    $('#mode').text(mode);
                    var pauseIcon = $('#pauseOther');
                    var pause = false;
                    if (mode === "paused") {
                        //otherUserMediaElement.pause();
                        //videoCapture();
                        pauseIcon.show();
                        $('#out').hide();
                        pause = true;
                    } else {
                        //otherUserMediaElement.play();
                        pauseIcon.hide();
                        $('#out').show();
                    }
                    // ideally if this is pause-start, we'd snapshot the video and pull a still
                    //otherUserMediaElement.srcObject.getTracks().forEach(t => t.enabled = !pause);
                }
            }
        }, 250);
    });
</script>
</body>
</html>